{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "convenient-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Tomita_Grammars import tomita_3\n",
    "from Training_Functions import make_train_set_for_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "frozen-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "target = tomita_3\n",
    "alphabet = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "offshore-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made train set of size: 2741 , of which positive examples: 1312\n",
      "2741\n",
      "[('', True), ('1', True), ('0', True), ('11', True), ('01', True), ('00', True), ('10', False), ('011', True), ('000', True), ('100', True)]\n"
     ]
    }
   ],
   "source": [
    "train_set = make_train_set_for_target(target,alphabet)\n",
    "\n",
    "print(len(train_set))\n",
    "print(list(train_set.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "interesting-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1}\n",
      "----------------------------------------------------\n",
      "{0: '0', 1: '1'}\n",
      "----------------------------------------------------\n",
      "{True: 0, False: 1}\n"
     ]
    }
   ],
   "source": [
    "num_chars = len(alphabet)\n",
    "\n",
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "#integer_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "#print(integer_encoded)\n",
    "#print(\"----------------------------------------------------\")\n",
    "#print(\"data length : \", len(integer_encoded))\n",
    "int2class = [True,False] # binary classifier for now\n",
    "class2int = {c:i for i,c in enumerate(int2class)}\n",
    "print(class2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "therapeutic-indiana",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': array([[1.],\n",
      "       [0.]]), '1': array([[0.],\n",
      "       [1.]])}\n",
      "[[0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "encodings = []\n",
    "for i in range(len(alphabet)):\n",
    "    pre_encoding = np.zeros((num_chars, 1))\n",
    "    pre_encoding[i] = 1\n",
    "    encodings.append(pre_encoding)\n",
    "encoding_dict = dict((i,  encodings[int(i)]) for i in alphabet)\n",
    "encoding_array = np.array(encodings)\n",
    "print(encoding_dict)\n",
    "print(encoding_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "opening-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1. - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "orange-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 5\n",
    "sequence_length = 10\n",
    "words = list(train_set.keys())\n",
    "batch_size = round((len(words) /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 10  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "located-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(word, target, h_prev):\n",
    "    # inputs here is a word with the alphabet eg, alphabet = '01', word = '0110011'    \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ps = {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for num, char in enumerate(word): # t is a \"time step\" and is used as a key(dic).\n",
    "        \n",
    "        xs[num] =  encoding_dict[char]\n",
    "        xs_reshaped = xs[num].reshape(num_chars, )\n",
    "        hs[num] = sigmoid(np.dot(np.dot(W_xh, xs_reshaped), h_prev).reshape(hidden_size, 1) + b_h) # hidden state. \n",
    "#         hs[num] = np.tanh(np.dot(np.dot(W_xh, xs_reshaped), h_prev).reshape(hidden_size, 1) + b_h) # hidden state. \n",
    "        \n",
    "    ys = np.dot(W_hy, hs[num]) + b_y # unnormalized log probabilities for next chars\n",
    "    ps = softmax(ys) # probability of '0' and '1'\n",
    "    p = ps[class2int[True]]\n",
    "    p = p if target == True else (1-p)\n",
    "    loss = -np.log(p) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "    return loss, ps, hs, xs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "adopted-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [0.70267745]\n",
      "ps  [[0.5047425]\n",
      " [0.4952575]]\n",
      "hs  {-1: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 0: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 1: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 2: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 3: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 4: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 5: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]]), 6: array([[0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5],\n",
      "       [0.5]])}\n",
      "xs  {0: array([[0.],\n",
      "       [1.]]), 1: array([[1.],\n",
      "       [0.]]), 2: array([[0.],\n",
      "       [1.]]), 3: array([[1.],\n",
      "       [0.]]), 4: array([[0.],\n",
      "       [1.]]), 5: array([[0.],\n",
      "       [1.]]), 6: array([[1.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "loss, ps, hs, xs = forwardprop(words[200], train_set[words[200]], h_prev)\n",
    "print(f'loss {loss}')\n",
    "print(f'ps  {ps}')\n",
    "print(f'hs  {hs}')\n",
    "print(f'xs  {xs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "computational-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, word, target, hs, xs):\n",
    "\n",
    "    dW_xh, dWhy = np.zeros_like(W_xh),  np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "    \n",
    "    dy = np.copy(ps)\n",
    "    dy[class2int[target]] -= 1\n",
    "\n",
    "    # reversed\n",
    "    for num, char in reversed(list(enumerate(word))):\n",
    "        dWhy += np.dot(dy, hs[num].T)\n",
    "        dby += dy \n",
    "        \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h.\n",
    "        dhraw = sigmoid_derivative(hs[num]) * dh\n",
    "        \n",
    "#         dhraw = (1 - hs[num] * hs[num]) * dh\n",
    "        \n",
    "        dbh += dhraw\n",
    "        x_h  = np.dot(hs[num], xs[num].T).reshape(1, hidden_size, num_chars).T \n",
    "        \n",
    "        dW_xh += np.dot(x_h, dhraw.T).T\n",
    "        \n",
    "        xs_reshaped = xs[num].reshape(num_chars, )\n",
    "        W_xh_xs = np.dot(W_xh, xs_reshaped)\n",
    "        \n",
    "        dhnext = np.dot(W_xh_xs, dhraw)\n",
    "        \n",
    "    for dparam in [dW_xh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dW_xh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "blessed-wednesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW_xh ==> [[[-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]\n",
      "  [-0.00040221 -0.00053407]]\n",
      "\n",
      " [[-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]\n",
      "  [-0.00103741 -0.0013852 ]]\n",
      "\n",
      " [[ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]\n",
      "  [ 0.00025143  0.00031243]]\n",
      "\n",
      " [[ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]\n",
      "  [ 0.0007      0.00093936]]\n",
      "\n",
      " [[-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]\n",
      "  [-0.00190149 -0.0025303 ]]\n",
      "\n",
      " [[-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]\n",
      "  [-0.00300271 -0.00398204]]\n",
      "\n",
      " [[ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]\n",
      "  [ 0.00228676  0.00305872]]\n",
      "\n",
      " [[ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]\n",
      "  [ 0.00133831  0.00176632]]\n",
      "\n",
      " [[ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]\n",
      "  [ 0.00182476  0.00239943]]\n",
      "\n",
      " [[ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]\n",
      "  [ 0.0066945   0.00894669]]]\n",
      "dWhy ==> [[ 1.76659876  1.76659876  1.76659876  1.76659876  1.76659876  1.76659876\n",
      "   1.76659876  1.76659876  1.76659876  1.76659876]\n",
      " [-1.76659876 -1.76659876 -1.76659876 -1.76659876 -1.76659876 -1.76659876\n",
      "  -1.76659876 -1.76659876 -1.76659876 -1.76659876]]\n",
      "dbh ==> [[-0.00187257]\n",
      " [-0.00484521]\n",
      " [ 0.00112772]\n",
      " [ 0.00327871]\n",
      " [-0.00886358]\n",
      " [-0.01396949]\n",
      " [ 0.01069097]\n",
      " [ 0.00620927]\n",
      " [ 0.00844839]\n",
      " [ 0.03128238]]\n",
      "dby ==> [[ 3.53319751]\n",
      " [-3.53319751]]\n"
     ]
    }
   ],
   "source": [
    "dW_xh, dWhy, dbh, dby = backprop(ps, words[200], train_set[words[200]], hs, xs)\n",
    "print(f'dW_xh ==> {dW_xh}')\n",
    "print(f'dWhy ==> {dWhy}')\n",
    "print(f'dbh ==> {dbh}')\n",
    "print(f'dby ==> {dby}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "packed-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5377997854458036\n",
      "0.3825749803673276\n",
      "0.4451202433649132\n",
      "0.4671175983958619\n",
      "0.44194731483214406\n",
      "0.4470014911806867\n",
      "0.5754134797530883\n",
      "0.5854919687652133\n",
      "0.5981901214800541\n",
      "0.5759484473479521\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-7e624e6e39e5>:39: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  word_loss = np.sum(word_losses)/len(word_losses)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.9022711681944894\n",
      "0.6558603915740002\n",
      "0.6541533990955574\n",
      "0.6330966308796308\n",
      "0.6022690731639062\n",
      "0.6119632510809614\n",
      "0.6666354122760728\n",
      "0.6515764163585345\n",
      "0.6470206182324745\n",
      "0.5783496806487703\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.8779497599553802\n",
      "0.6949207690559887\n",
      "0.6725422600938596\n",
      "0.6587039839180835\n",
      "0.6366850240907033\n",
      "0.6465981258287353\n",
      "0.6861850727642846\n",
      "0.6706934015528281\n",
      "0.6642709234340111\n",
      "0.5835366003056582\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.8538813266756214\n",
      "0.7123050837074041\n",
      "0.6774243189157021\n",
      "0.668278567832291\n",
      "0.6508598123304958\n",
      "0.6602162532858873\n",
      "0.6926024987605806\n",
      "0.6780048309624815\n",
      "0.6717773767122671\n",
      "0.5882998149362437\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.8373640453521395\n",
      "0.7217185218202105\n",
      "0.6794570291738457\n",
      "0.6729664830847086\n",
      "0.6581984052544089\n",
      "0.6668423276208918\n",
      "0.6949748326451113\n",
      "0.6810101904656336\n",
      "0.6747423874109495\n",
      "0.5911237787502917\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mW_xh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "loss_values = []\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    \n",
    "    \n",
    "    batches_loss = []\n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        batch = words[b*batch_size:(b+1)*batch_size]\n",
    "        word_losses = [] \n",
    "        for word in batch[1:]:\n",
    "            loss, probabilities, hidden_states, chars = forwardprop(word, train_set[word], h_prev)\n",
    "            #print(loss)\n",
    "            #print(probabilities)\n",
    "            word_losses.append(loss)\n",
    "            dW_xh, dWhy, dbh, dby = backprop(probabilities, word, train_set[word], hidden_states, chars)\n",
    "            \n",
    "            #perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip([W_xh, W_hy, b_h, b_y], \n",
    "                                          [dW_xh, dWhy, dbh, dby], \n",
    "                                          [mW_xh, mWhy, mbh, mby]):\n",
    "                mem += dparam * dparam # elementwise\n",
    "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "            \n",
    "#             W_xh -= learning_rate * dW_xh\n",
    "#             W_hy  -= learning_rate * dWhy\n",
    "#             b_h   -= learning_rate * dbh\n",
    "#             b_y   -= learning_rate * dby\n",
    "        \n",
    "        \n",
    "        word_loss = np.sum(word_losses)/len(word_losses)\n",
    "        print(word_loss)\n",
    "        batches_loss.append(word_loss)\n",
    "    loss_value = np.sum(batches_loss)/batch_size\n",
    "    #print(loss_value)\n",
    "    loss_values.append(loss_value)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-permission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
